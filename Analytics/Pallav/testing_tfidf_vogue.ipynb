{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# natural language processing: n-gram ranking\n",
    "import re\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all links\n",
    "curr_path = os.path.abspath(\"testing_tfidf_vogue.ipynb\")\n",
    "df_path = os.path.abspath(os.path.join(curr_path, \"../../..\", \"Read_Files/fashion_intern_forecasting_website_list.csv\"))\n",
    "df = pd.read_csv(df_path)\n",
    "n = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all stopwords\n",
    "additional_stopwords = []\n",
    "stopwords = nltk.corpus.stopwords.words('english') + additional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to cleanup text and find words in it\n",
    "def extract_words_from_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean up the passed text.\\n\n",
    "    All words that are not designated as a stop word are lemmatized afte encoding and basic regex parsing is performed.\\n\n",
    "    \\n\n",
    "    Parameters:\n",
    "    text - Text to be worked with\n",
    "    \"\"\"\n",
    "\n",
    "    # lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    # text cleaning\n",
    "    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore').lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "\n",
    "    # word list\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = []\n",
    "for i in range(n):\n",
    "    # blog link\n",
    "    blog_link = df[\"Website URL\"][i]\n",
    "\n",
    "    # getting page content\n",
    "    html_response = requests.get(blog_link)\n",
    "    if html_response.status_code != 200:\n",
    "        continue\n",
    "\n",
    "    # get soup object\n",
    "    html_text = html_response.text\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "\n",
    "    # get all text\n",
    "    all_text = [element.text.strip() for element in soup.find_all([\"p\", \"span\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])]\n",
    "\n",
    "    document_list.append(extract_words_from_text(text=\" \".join(all_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "473418c0e77683684e48f36ee0a4eb17769bc02766a503d3556a3480c1d685e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
